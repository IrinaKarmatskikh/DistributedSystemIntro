\newpage
\section {Билет 5. Оптимизация запросов, построение и оценка планов запросов.}

\begin{center}
\textit{\underline{Операции в базах данных.}}
\end{center}

Операции существуют 2-ух типов: 
\begin{itemize}
    \item \hypertarget{DML_label}{DML} - это аббревиатура от языка манипулирования данными. Он используется для извлечения, хранения, изменения, удаления, вставки и обновления данных в базе данных.

\textit{Примеры:} операторы SELECT, UPDATE, INSERT, DELETE, MERGE

\item DDL - это аббревиатура языка определения данных. Он используется для создания и изменения структуры объектов базы данных.

\textit{Примеры}: операторы CREATE, ALTER, DROP
\end{itemize}

Приведем синтаксис оператора SELECT: 

SELECT поля

FROM таблица 

WHERE условия 

С оператором SELECT дополнительно можно применять GROUP BY и HAVING.  

\begin{center}
\textit{\underline{Способы соединения таблиц.}}
\end{center}

Хотим понять каким способом делать соединения таблиц. Пусть хотим соединить 2 таблицы по id, те по A.id = B.id с условием A.a = x, B.b = y.

\textit{Способ 1}: \textbf{nested loop (вложенный цикл)}

Выбрать все данные из одной таблицы (например А по условию A.a= x) после этого для каждого id выбрать все подходящие из B (по индексу и значению B.b = y). По фатку это 2 цикла вложенных в друг друга.

Сложность: $N^2$. Если критерий x является очень хороший, то сложность становиться N. 

\textit{Способ 2}: \textbf{merge join (слияние)}

Взять наши данные и отфильтровать отдельно по условию x в таблице А, по условию y в таблице В, а дальше отсортировать и после этого соединить два отсортированных массива.

Сложность: $NLogN$. Если критерий x является очень хороший, а таблица В большая, то сложность останется такой же. 

Из-за этого нельзя сказать, какой способ (1 или 2) лучше. 

\textit{Способ 3}: \textbf{hash join (хеширование)}

При соединении хешированием строки одного набора помещаются в
хеш-таблицу, содержащуюся в памяти, а строки из второго набора
перебираются, и для каждой из них проверяется наличие
соответствующих строк в хеш-таблице.

Ключом хеш-таблицы является тот столбец, по которому выполняется
соединение наборов строк.

Такой способ эффективен для больших выборок. 


\begin{center}
\textit{\underline{Способы доступа к данным таблицы.}}
\end{center}

Теперь мы хотим выполнять запросы к одной таблице. У нас остается A.a= x. 

\textit{Способ 1}: \textbf{full scan} - проходит таблицу целиком. 

\textit{Способ 2}: \textbf{index} - используем индекс, если нужное значение заиндексировано и мы ищем какой-то диапазон. 

\textit{Пример}: номер зачетки заиндексирован, и мы просто хотим найти номер зачетки, который равен x или находится в каком-то диапазоне. 

\textit{Способ 3}: \textbf{index scan} - предположим, что значение заиндексировано, а мы, например, хотим выбирать по значениям какой-то функции от значения в таблице, сам индекс такого не даст. Так что нам придется в любом случае читать данные. Вся запись весит много, а вот индекс, который хранит данные только одного значения, по факту весит гораздо меньше. 

\textit{Пример}: заиндексирован номер зачетки, хотим получить человека с номером зачетки таким, что Sin от номера зачетки лежит в [0.5, 0.6], сам индекс такого не даст. Индекс хранит только номера зачеток, а полная запись содержит ФИО, место рождения, дату рождения и еще много всего. Соответственно, занимает много памяти и времени, если все читать из памяти для поиска, а в индексе мы читаем только номер зачетки, что весит гораздо меньше -> быстрее. 

\begin{center}
\textit{\underline{План выполнения запроса. Оптимизатор.}}
\end{center}

Пусть у нас есть запрос, в котором присутсвуют таблицы A, B, C, D. Нужно построить план выполнения запроса, которые выглядит как дерево (Рис. \ref{fig:plan1}). Стоит помнить, что это не синтаксический разбор. Тут мы решили отсканировать A full scan, B по индексу, а потом сделать между ними hash join. 

\begin{figure}[h!]
    \centering
    \includegraphics[scale = 0.5]{5/plan.jpg}
    \caption{}
    \label{fig:plan1}
\end{figure}

Скорость выполнения ОЧЕНЬ сильно зависит от оптимальности плана запроса.
\\[20pt]
Для выбора плана запроса в базе данных сущестсвует \textbf{оптимизатор}. Он получает на вход разобранный синтаксически SELECT и выдает план выполнения запроса (внутри оптимизатора еще проверяются права доступа). 

\textbf{Виды оптимизаторов:} 
\begin{itemize}
    \item \textbf{Rule (cинтаксический)}. Основан на словарях, для того, чтобы выбрать лучший план запроса просматривает все условия, которые фигурируют в запросе и проводит анализ без учета данных (только на объялении структуры). Например в нем прошито правило, что индексный поиск по уникальному индексу - наилучший, после него идет не уникальный индекс, потом index scan, а потом full scan. Правил около 15, но суть одна (по факту прописаны приоритеты). Работает это очень быстро, но наилучший план он выбрать не может.
    
Почему плохой оптимизатор: мы хотим найти в базе студентов Иванова Рамиля, тогда нам выгодно в первую очередь искать по имени, так как Ивановых скорее всего много. Этот оптимизатор такое не учитывает. 

\item \textbf{Cost}. Такой вид оптимизатора строит много планов выполнения запроса и считает их стоимость. По факту делает "виртуальное" выполнение запроса. У него есть оценки сколько понадобится сделать действий, чтобы произвести какой-либо способ доступа. 
    
\textit{Пример:} в оптимизаторе есть информация, что в таблице 1к блоков и в ней 1 миллион записей при этом уникальных 100к, тогда full scan должен прочитать все блоки и выбрать где-то 10 записей. Если же будет использоваться индекс, то нужно будет прочитать 10 блоков и выбрать 10 записей. 
    
Так для каждого плана появляется цена выполнения. Рассмотрим модель работы Oracle DB: при расчете стоимости все операции (чтение диска, стоимость сортировки, процессорное время...) конвертируются в некоторую условную 'валюту', которая называется \textbf{логическим чтением блоков}. Стоимость высчитывается в этой валюте. Часто планы запроса хранятся, чтобы не пересчитываться много раз. 
    
Сложности: 
    
1) количество вариантов возможных планов запросов. Из-за этого оптимизатор работает классическими алгоритмами частичного обхода дерева (основная идея: берутся наилучшие ветки, начинаем спуск, если ветка оказалась плохой, то мы ее откидываем и дальше не проверяем). И еще можно поставить ограничение на количество вариантов. 
    
2) Где брать данные, чтобы понимать, сколько мы получим записей после какой-то выборки. Если мы выбираем по фиксированным условиям (имя = Петр), то помогает построение гистограмм: по каждому столбцу строим общие характеристики и строим гистограммы, сколько значений попадают в какой-то диапазон (на букву П у нас 8млн записей, на букву К 10млн и тд). Это частично решает проблему (так как строить уникальные гистограммы слишком затратно). 
    
Статистика не работает, если мы используем ограничения, которые невозможно заранее просчитать. Таких бывает 3 вида: 
\begin{enumerate}
    \item Для столбцов, которые имеют близкие значения с очень разными частотами.
    \item Использование ограничений, которые содержат функцию (хотим выбрать студентов у которых третий знак после запятой в синусе от веса студента равен 3, понятно, что гистограмма бесполезна).
    \item (самый важный для нас) Если мы имеем дело с доступом к удаленным данным: получаем данные с другого сервера, но статистики его мы не знаем.
\end{enumerate}

Есть два подхода к решению таких сложностей: 

\begin{enumerate}
    \item Семплирование - получение примера данных. (Например в Oracle DB: A SAMPLE (5) - хотим считать из таблицы 5\% случайных блоков). Берутся примеры данных из всех таблиц, и на них проверяются условия выборки. Потом расширяем полученные данные на всю таблицу и считаем, что получили всю статистику (для удаленных данных просто просим прислать какой-то процент блоков и далее аналогично). Способ хороший так как дает +- точные результаты, но есть недостаток - стоимость подготовки плана (может присылаться очень много данных).
    \item Перестраивание дерева на ходу. Реализуется сложно, но принцип простой:  смотрим на картинку плана запроса. Мы предполагаем, что при объединении HJ А и В мы получим примерно 4 записи (не стоимость) и тогда выше стоящий NL - оптимальный вариант. Начали выполнять, в момент соединения А и В HJ мы получили 400к записей и в таком случае система останавливает выполнение запроса и производит пересчет частей, которые мы еще не выполнили.
\end{enumerate}
\end{itemize}


\begin{center}
\textit{\underline{Переписывание.}}
\end{center}

Существует еще один метод оптимизации запроса - \textbf{переписывание}. 

Вспоминаем три структуры данных: 

\begin{enumerate}
    \item \textit{таблицы} - просто хранят данные;
    \item  \textit{представления} - хранят запросы к таблицам (возможно, в виде плана запроса);
    \item \textit{материальные представления (мп)} - хранят результат выполнения какого-то запроса. 
\end{enumerate}

Нас интересуют материальные представения. У мп есть метка вычисления (мы можем сказать, когда менялись таблицы и когда было посчитано материальное представление). Смотрим на рисунок плана (Рис. \ref{fig:plan1}). Пусть мы захотели отдельно посмотреть HJ для A и В. Если HJ был сделан после изменения таблиц, то мы можем удалить часть дерева ниже HJ и подставить туда материальное представление (Рис. \ref{fig:plan2}). Так мы сокращаем размер вычислений.  

 \begin{figure}[h!]
     \centering
     \includegraphics[scale = 0.5]{5/plan2.jpg}
     \caption{После добавления МП}
     \label{fig:plan2}
 \end{figure}
 
\begin{center}
\textit{\underline{Механизмы получения данных из внешней системы.}}
\end{center}

Существуют следующие механизмы получения данных из внешней системы (терминология  Oracle DB)
\begin{itemize}
    \item  dblink -  связывают бд одного типа (умеют все делать так как все друг о друге знают);
    \item внешние таблицы - если надо подключить статические данные (например csv файл). Может быть медленным, особо ничего нет;
    \item шлюзы (ODBC).
\end{itemize}
