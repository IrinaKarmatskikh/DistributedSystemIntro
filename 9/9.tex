\newpage
\section {Билет 9. IP, TCP, UDP, VPN, NAT}

\href{https://ru.wikipedia.org/wiki/IP}{IP} - специальный протокол,позволяющий осуществлять передачу сообщений между устройствами, находящимися в одной сети посредством идентификации этих устройств 4-байтовыми кодами (адресами).

Его работу можно полностью сравнить с работой Почты России. Если положить письмо в ящик, то оно \textbf{может быть} дойдёт до адресата. При этом, если положить 10 писем в ящик, то какое-то количество из них, наверняка, дойдёт, однако \textbf{неизвестно, в каком порядке}.

Естественно, в таких условиях программам работать не оч, поэтому на базе протокола IP были придуманы более хитрые протоколы, про которые мы и поговорим далее.

\bigskip
\href{https://ru.wikipedia.org/wiki/UDP}{UDP} - протокол на базе IP-протокола, позволяющий обмениваться данными. Работает следующим образом: пакет данных разбивается на блоки, которые по размеру подходят для передачи по IP-протоколу, и отправляет эти блоки, пронумеровав их. На стороне адресата эти блоки посредством нумерации собираются обратно в пакет данных.

UDP \textbf{не поддерживает} подтверждение о получении пакета, то есть, что-то, возможно, не дойдёт. Однако внутри одного блока данных UDP гарантирует, что все данные дойдут в целостности (если они вообще дойдут).

Используется в основном в стриминговых передачах (например, онлайн-трансляция видео или аудио, передача показаний датчиков в реальном времени). Основная идея - в отличие от TCP, неважно, если какие-то данные не дошли. Ну будет пробел и будет (не показывать же вам не дошедшие кадры из фильма через 5 секунд, как вы уже этот момент просмотрели).

\bigskip
\href{https://ru.wikipedia.org/wiki/Transmission_Control_Protocol}{TCP} - более сложный протокол передачи данных. Всё так же режет пакет данных на более мелкие блоки и нумерует их, однако отправка более хитрая.

TCP имеет буфер (для пример скажем, длины 4, хотя в реальности он гораздо больше), в котором размещены блоки с данными. Он отправляет данные из буфера и (в отличие от UDP!) ждёт.

Когда адресат получает, скажем, блок данных №3, он посылает подтверждение, что третий блок получен и контрольная сумма сошлась. После этого TCP убирает из буфера блок №3 и помещает туда следующий блок (в нашем примере, блок №5). По мере того, как приходят подтверждения, блоки из буфера убираются и заменяются на новые.

Если на блок долго не было подтверждения, то этот блок отправляется ещё раз. Передача завершается только тогда, когда на все блоки из пакета данных пришло подтверждение. Таким образом, можно гарантировать целостность передачи всего пакета данных (или узнать, какие именно блоки не дошли).

TPC применяется, например, при передаче файлов или просто каких-то данных в реляционных системах.

\bigskip
\textcolor{blue}{Я хз, нужно ли то, что будет тут далее про разбиение данных, но пусть будет, вроде как это всё ещё часть про TCP.}

При передаче данных возникает вопрос, насколько мелко стоит делить пакеты на блоки. Ведь при ошибке, скажем, 1 раз в 1000 байт, если делить данные на блоки по 100 байт, ошибка будет в среднем в 1 из 10 блоков (то есть, в среднем, повторно пересылать придётся около 10\% данных). А если делить на блоки по 500 байт, то ошибка будет в каждом втором блоке.

С одной стороны, общий принцип пересылки - чем меньше пакет, тем больше вероятность, что он дойдёт. 

С другой стороны, в случае TCP-протокола, больше блоков = больше ответных подтверждений. К тому же, не стоит забывать, что каждый блок имеет заголовок (информация о том, кто отправитель, кто адресат, номер блока, общее кол-во блоков и т.д.) фиксированного размера, не зависящего от разбиения. Поэтому, если размер заголовка, скажем, 100 байт, при разбиении пакета данных на блоки по 500 байт, итоговый размер пересланных данных вырастет на $\frac{1}{5}$. А если делить на блоки по 100 байт, то итоговый размер пересылки вырастет в 2 раза.

\textbf{Мораль проста надо делить данные на как можно бОльшие блоки.}

Одной отличительной чертой TCP является то, что он умеет адаптироваться к качеству сети. То есть, по мере пересылок данных TCP может просчитывать, как часто возникают ошибки в передаче, и варьировать оптимальный размер блоков, на которые разбивает пакеты. Для этого он постепенно повышает размер блоков, пока не дойдёт до \textit{критической секции}, когда повторные пересылки блоков становятся непозволительно частыми из-за ошибок сети. Тогда он делает размер блоков чуть поменьше, чтоб ошибки опять не так сильно душили, и держит эту - оптимальную - планку.

Если качество сети какое-то время было норм, а потом резко упало (гроза началась или птицы таджик не тот провод перерезал), то естественно, что оптимальный размер блоков сразу станет неоптимальным и начнёт часто выдавать ошибки. В таком случае TCP делает несколько попыток передать данные такими блоками, какие были, а после на основе статистики ошибок сделает перерасчёт разбиения пакета, чтоб опять всё было оптимально. Когда качество сети опять поднимется, TCP это заметит и начнёт постепенно увеличивать размер блоков до оптимального.

Если сеть часто сбоит и работает нестабильно, можно самому поставить искусственную планку - \href{https://ru.wikipedia.org/wiki/Maximum_transmission_unit}{MTU}. Это параметр, определяющий максимальный размер блока при разбиении пакета данных. Больше него TCP не будет делать блоки, даже если это было бы оптимальнее.

\bigskip 
Представим бытовую ситуацию: у нас дома есть WI-Fi роутер, подключённый к сети Интернет, и к этому роутеру в квартире подключены ПК, ноут, планшет, телефоны и много кто ещё. С точки зрения внешней сети (Интернет) существует лишь роутер со своим IP-адресом. С точки зрения роутера есть внешняя сеть + все девайсы, которые находятся дома (у каждого из них есть свой локальный адрес, уникальный в этой локальной домашней сети). Тогда как нам отделять входящие и исходящие запросы, предназначаемые для ноута, ПК и прочих девайсов по отдельности?

В решении этой задачи и заключается суть технологии \href{https://ru.wikipedia.org/wiki/NAT}{NAT}. Принимая пакет от локального компьютера, роутер смотрит на IP-адрес назначения. Если это локальный адрес, то пакет пересылается другому локальному компьютеру. Если нет, то пакет надо переслать наружу в интернет. Но ведь обратным адресом в пакете указан локальный адрес компьютера, который из интернета будет недоступен. Поэтому роутер «на лету» транслирует (подменяет) обратный IP-адрес пакета на свой внешний (видимый из интернета) IP-адрес и меняет номер порта (чтобы различать ответные пакеты, адресованные разным локальным компьютерам). Комбинацию, нужную для обратной подстановки, роутер сохраняет у себя во временной таблице. Через некоторое время после того, как клиент и сервер закончат обмениваться пакетами, роутер сотрёт у себя в таблице запись об n-м порте за сроком давности.

NAT меняет лишь заголовок и не трогает само "тело" передачи (посылает пакеты в таком виде, в каком их получил). Из-за этого сама по себе пересылка данных является незащищённой (то есть, если вы захотите переслать пароли с одного сервера на другой через Интернет, то потенциальный злоумышленник сможет их прочитать где-то по пути передачи).

\bigskip    
Для построения защищённых каналов связи используется \href{https://ru.wikipedia.org/wiki/VPN}{VPN}. Представим, что у нас есть два сервера, у каждого из которых какой-то свой закрытый контур (внутри которого уже используется, например, NAT), и мы хотим провести между ними соединение через Интернет. VPN помогает установить защищённый канал, проходя через который, данные будут шифроваться на входе и расшифровываться на выходе.
Таким образом, даже если потенциальный злоумышленник захочет прочитать эти данные (да, он всё ещё может где-то по пути пересылки данных увидеть их), то не сможет, ведь они зашифрованы. Недостатком VPN, очевидно, является скорость, ведь шифровать и расшифровывать данные - не самая быстрая операция.